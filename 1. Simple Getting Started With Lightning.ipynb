{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning factors DL/ML code into three types:\n",
    "\n",
    " * Research code\n",
    "\n",
    " * Engineering code\n",
    "\n",
    " * Non-essential code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "\n",
    "class LitMNIST(pl.LightningModule):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # mnist images are (1, 28, 28) (channels, width, height)\n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, channels, width, height = x.size()\n",
    "\n",
    "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "    x = x.view(batch_size, -1)\n",
    "\n",
    "    # layer 1\n",
    "    x = self.layer_1(x)\n",
    "    x = torch.relu(x)\n",
    "\n",
    "    # layer 2\n",
    "    x = self.layer_2(x)\n",
    "    x = torch.relu(x)\n",
    "\n",
    "    # layer 3\n",
    "    x = self.layer_3(x)\n",
    "\n",
    "    # probability distribution over labels\n",
    "    x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this is a LightningModule instead of a torch.nn.Module. A LightningModule is equivalent to a PyTorch Module except it has added functionality. However, you can use it EXACTLY the same as you would a PyTorch Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3678e+34, -2.1916e+34,  0.0000e+00, -3.0966e+32, -9.2373e+33,\n",
       "         -9.9944e+33, -6.1999e+33, -8.8007e+33, -2.2410e+34, -1.5735e+34]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LitMNIST()\n",
    "x = torch.Tensor(1, 1, 28, 28)\n",
    "out = net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The Lightning Module organizes your dataloaders and data processing as well. Here’s the PyTorch code for loading MNIST\n",
    "```python\n",
    "class LitMNIST(pl.LightningModule):\n",
    "\n",
    "  def prepare_data(self):\n",
    "    # stuff here is done once at the very beginning of training\n",
    "    # before any distributed training starts\n",
    "\n",
    "    # download stuff\n",
    "    # save to disk\n",
    "    # etc...\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # data transforms\n",
    "    # dataset creation\n",
    "    # return a DataLoader\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# transforms\n",
    "# prepare transforms standard to MNIST\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# data\n",
    "mnist_train = MNIST(os.getcwd(), train=True, download=True)\n",
    "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "\n",
    "class LitMNIST(pl.LightningModule):\n",
    "\n",
    "  def prepare_data(self):\n",
    "    # download only\n",
    "    MNIST(os.getcwd(), train=True, download=True)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # no download, just transform\n",
    "    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    mnist_train = MNIST(os.getcwd(), train=True, download=False,\n",
    "                        transform=transform)\n",
    "    return DataLoader(mnist_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "In Lightning optimizers are under the configure_optimizers method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(pl.LightningModule):\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step\n",
    "\n",
    "Training step in pytorch typically looks like below:\n",
    "```python\n",
    "for epoch in epochs:\n",
    "    for batch in data:\n",
    "        # TRAINING STEP\n",
    "        # ....\n",
    "        # TRAINING STEP\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "In Lightning, everything that is in the training step gets organized under the training_step function in the LightningModule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(pl.LightningModule):\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    logits = self(x)\n",
    "    loss = F.nll_loss(logits, y)\n",
    "    return {'loss': loss}\n",
    "    # return loss (also works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Combining all Togeter\n",
    "\n",
    "So far we defined 4 key ingredients in pure PyTorch but organized the code inside the LightningModule.\n",
    "\n",
    "* Model.\n",
    "\n",
    "* Training data.\n",
    "\n",
    "* Optimizer.\n",
    "\n",
    "* What happens in the training loop.\n",
    "\n",
    "For clarity, we’ll recall that the full LightningModule now looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, channels, width, height = x.size()\n",
    "    x = x.view(batch_size, -1)\n",
    "    x = self.layer_1(x)\n",
    "    x = torch.relu(x)\n",
    "    x = self.layer_2(x)\n",
    "    x = torch.relu(x)\n",
    "    x = self.layer_3(x)\n",
    "    x = torch.log_softmax(x, dim=1)\n",
    "    return x\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)\n",
    "    return DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    logits = self(x)\n",
    "    loss = F.nll_loss(logits, y)\n",
    "\n",
    "    # add logging\n",
    "    logs = {'loss': loss}\n",
    "    return {'loss': loss, 'log': logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "When we added the log key in the return dictionary it went into the built in tensorboard logger. But you could have also logged by calling:\n",
    "\n",
    "```python\n",
    "def training_step(self, batch, batch_idx):\n",
    "    # ...\n",
    "    loss = ...\n",
    "    self.logger.summary.scalar('loss', loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:VISIBLE GPUS: 0\n",
      "/home/av6101604/env/lib64/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py:106: UserWarning: Hyperparameter logging is not available for Torch version 1.2.0. Skipping log_hyperparams. Upgrade to Torch 1.3.0 or above to enable hyperparameter logging.\n",
      "  f\"Hyperparameter logging is not available for Torch version {torch.__version__}.\"\n",
      "INFO:lightning:\n",
      "  | Name    | Type   | Params\n",
      "-------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d256da0c4ebc449e8764ace215edfaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/av6101604/env/lib64/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: Displayed epoch numbers in the progress bar start from \"1\" until v0.6.x, but will start from \"0\" in v0.8.0.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/av6101604/env/lib64/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "model = LitMNIST()\n",
    "trainer = Trainer(gpus=1,max_epochs=4)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
